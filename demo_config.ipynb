{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lyu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nypost_gun_page1to10000_timeto.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     11\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data/nypost\u001B[39m\u001B[38;5;124m'\u001B[39m, fname))\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx, row \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39miterrows():\n\u001B[0;32m---> 13\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m st \u001B[38;5;129;01min\u001B[39;00m \u001B[43msent_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m     14\u001B[0m         sts_list \u001B[38;5;241m=\u001B[39m gensim\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39msimple_preprocess(st)\n\u001B[1;32m     15\u001B[0m         hist \u001B[38;5;241m=\u001B[39m Counter(sts_list)\n",
      "File \u001B[0;32m~/Opt/anaconda3/envs/m1_torch/lib/python3.8/site-packages/nltk/tokenize/__init__.py:107\u001B[0m, in \u001B[0;36msent_tokenize\u001B[0;34m(text, language)\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001B[39;00m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;124;03musing NLTK's recommended sentence tokenizer\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m:param language: the model name in the Punkt corpus\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    106\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m load(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizers/punkt/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlanguage\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pickle\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 107\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Opt/anaconda3/envs/m1_torch/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1281\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.tokenize\u001B[0;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[1;32m   1277\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m, realign_boundaries: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m   1278\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1279\u001B[0m \u001B[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001B[39;00m\n\u001B[1;32m   1280\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1281\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msentences_from_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrealign_boundaries\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/Opt/anaconda3/envs/m1_torch/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1341\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.sentences_from_text\u001B[0;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[1;32m   1332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msentences_from_text\u001B[39m(\n\u001B[1;32m   1333\u001B[0m     \u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m, realign_boundaries: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1334\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m   1335\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1336\u001B[0m \u001B[38;5;124;03m    Given a text, generates the sentences in that text by only\u001B[39;00m\n\u001B[1;32m   1337\u001B[0m \u001B[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001B[39;00m\n\u001B[1;32m   1338\u001B[0m \u001B[38;5;124;03m    True, includes in the sentence closing punctuation that\u001B[39;00m\n\u001B[1;32m   1339\u001B[0m \u001B[38;5;124;03m    follows the period.\u001B[39;00m\n\u001B[1;32m   1340\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1341\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [text[s:e] \u001B[38;5;28;01mfor\u001B[39;00m s, e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001B[0;32m~/Opt/anaconda3/envs/m1_torch/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1341\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msentences_from_text\u001B[39m(\n\u001B[1;32m   1333\u001B[0m     \u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m, realign_boundaries: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1334\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m   1335\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1336\u001B[0m \u001B[38;5;124;03m    Given a text, generates the sentences in that text by only\u001B[39;00m\n\u001B[1;32m   1337\u001B[0m \u001B[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001B[39;00m\n\u001B[1;32m   1338\u001B[0m \u001B[38;5;124;03m    True, includes in the sentence closing punctuation that\u001B[39;00m\n\u001B[1;32m   1339\u001B[0m \u001B[38;5;124;03m    follows the period.\u001B[39;00m\n\u001B[1;32m   1340\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1341\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [text[s:e] \u001B[38;5;28;01mfor\u001B[39;00m s, e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001B[0;32m~/Opt/anaconda3/envs/m1_torch/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1329\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.span_tokenize\u001B[0;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[1;32m   1327\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m realign_boundaries:\n\u001B[1;32m   1328\u001B[0m     slices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_realign_boundaries(text, slices)\n\u001B[0;32m-> 1329\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m slices:\n\u001B[1;32m   1330\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m (sentence\u001B[38;5;241m.\u001B[39mstart, sentence\u001B[38;5;241m.\u001B[39mstop)\n",
      "File \u001B[0;32m~/Opt/anaconda3/envs/m1_torch/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1459\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer._realign_boundaries\u001B[0;34m(self, text, slices)\u001B[0m\n\u001B[1;32m   1446\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1447\u001B[0m \u001B[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001B[39;00m\n\u001B[1;32m   1448\u001B[0m \u001B[38;5;124;03mshould otherwise be included in the same sentence.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1456\u001B[0m \u001B[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001B[39;00m\n\u001B[1;32m   1457\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1458\u001B[0m realign \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1459\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sentence1, sentence2 \u001B[38;5;129;01min\u001B[39;00m _pair_iter(slices):\n\u001B[1;32m   1460\u001B[0m     sentence1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mslice\u001B[39m(sentence1\u001B[38;5;241m.\u001B[39mstart \u001B[38;5;241m+\u001B[39m realign, sentence1\u001B[38;5;241m.\u001B[39mstop)\n\u001B[1;32m   1461\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sentence2:\n",
      "File \u001B[0;32m~/Opt/anaconda3/envs/m1_torch/lib/python3.8/site-packages/nltk/tokenize/punkt.py:324\u001B[0m, in \u001B[0;36m_pair_iter\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m    322\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    323\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m--> 324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m el \u001B[38;5;129;01min\u001B[39;00m iterator:\n\u001B[1;32m    325\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m (prev, el)\n\u001B[1;32m    326\u001B[0m     prev \u001B[38;5;241m=\u001B[39m el\n",
      "File \u001B[0;32m~/Opt/anaconda3/envs/m1_torch/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1432\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer._slices_from_text\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m   1430\u001B[0m last_break \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   1431\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m match, context \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_match_potential_end_contexts(text):\n\u001B[0;32m-> 1432\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext_contains_sentbreak\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1433\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mslice\u001B[39m(last_break, match\u001B[38;5;241m.\u001B[39mend())\n\u001B[1;32m   1434\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m match\u001B[38;5;241m.\u001B[39mgroup(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnext_tok\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   1435\u001B[0m             \u001B[38;5;66;03m# next sentence starts after whitespace\u001B[39;00m\n",
      "File \u001B[0;32m~/Opt/anaconda3/envs/m1_torch/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1480\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.text_contains_sentbreak\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m   1476\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1477\u001B[0m \u001B[38;5;124;03mReturns True if the given text includes a sentence break.\u001B[39;00m\n\u001B[1;32m   1478\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1479\u001B[0m found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# used to ignore last token\u001B[39;00m\n\u001B[0;32m-> 1480\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m tok \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_annotate_tokens(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tokenize_words(text)):\n\u001B[1;32m   1481\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m found:\n\u001B[1;32m   1482\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/Opt/anaconda3/envs/m1_torch/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1623\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer._annotate_second_pass\u001B[0;34m(self, tokens)\u001B[0m\n\u001B[1;32m   1617\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1618\u001B[0m \u001B[38;5;124;03mPerforms a token-based classification (section 4) over the given\u001B[39;00m\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;124;03mtokens, making use of the orthographic heuristic (4.1.1), collocation\u001B[39;00m\n\u001B[1;32m   1620\u001B[0m \u001B[38;5;124;03mheuristic (4.1.2) and frequent sentence starter heuristic (4.1.3).\u001B[39;00m\n\u001B[1;32m   1621\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1622\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m token1, token2 \u001B[38;5;129;01min\u001B[39;00m _pair_iter(tokens):\n\u001B[0;32m-> 1623\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_second_pass_annotation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1624\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m token1\n",
      "File \u001B[0;32m~/Opt/anaconda3/envs/m1_torch/lib/python3.8/site-packages/nltk/tokenize/punkt.py:1650\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer._second_pass_annotation\u001B[0;34m(self, aug_tok1, aug_tok2)\u001B[0m\n\u001B[1;32m   1642\u001B[0m tok_is_initial \u001B[38;5;241m=\u001B[39m aug_tok1\u001B[38;5;241m.\u001B[39mis_initial\n\u001B[1;32m   1644\u001B[0m \u001B[38;5;66;03m# [4.1.2. Collocation Heuristic] If there's a\u001B[39;00m\n\u001B[1;32m   1645\u001B[0m \u001B[38;5;66;03m# collocation between the word before and after the\u001B[39;00m\n\u001B[1;32m   1646\u001B[0m \u001B[38;5;66;03m# period, then label tok as an abbreviation and NOT\u001B[39;00m\n\u001B[1;32m   1647\u001B[0m \u001B[38;5;66;03m# a sentence break. Note that collocations with\u001B[39;00m\n\u001B[1;32m   1648\u001B[0m \u001B[38;5;66;03m# frequent sentence starters as their second word are\u001B[39;00m\n\u001B[1;32m   1649\u001B[0m \u001B[38;5;66;03m# excluded in training.\u001B[39;00m\n\u001B[0;32m-> 1650\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43m(\u001B[49m\u001B[43mtyp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnext_typ\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollocations\u001B[49m:\n\u001B[1;32m   1651\u001B[0m     aug_tok1\u001B[38;5;241m.\u001B[39msentbreak \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1652\u001B[0m     aug_tok1\u001B[38;5;241m.\u001B[39mabbr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "sentences_cnn = []\n",
    "counts_general = dict()\n",
    "counts_cnn = dict()\n",
    "counts_nypost = dict()\n",
    "counts_general[0] = 0\n",
    "counts_cnn[0] = 0\n",
    "counts_nypost[0] = 0\n",
    "for fname in os.listdir('./data/nypost/'):\n",
    "    if len(fname) > 12:\n",
    "        print(fname)\n",
    "        df = pd.read_csv(os.path.join('./data/nypost', fname))\n",
    "        for idx, row in df.iterrows():\n",
    "            for st in sent_tokenize(str(row['text'])):\n",
    "                sts_list = gensim.utils.simple_preprocess(st)\n",
    "                hist = Counter(sts_list)\n",
    "                for k in hist:\n",
    "                    counts_general[0] += 1\n",
    "                    counts_nypost[0] += 1\n",
    "                    counts_general[k] = hist[k] + counts_general.get(k, 0)\n",
    "                    counts_nypost[k] = hist[k] + counts_general.get(k, 0)\n",
    "                sentences_cnn.append(sts_list)\n",
    "\n",
    "sentences_nypost = []\n",
    "for fname in os.listdir('./data/CNN/'):\n",
    "    if len(fname) > 12:\n",
    "        print(fname)\n",
    "        df = pd.read_csv(os.path.join('./data/CNN', fname))\n",
    "        for idx, row in df.iterrows():\n",
    "            for st in sent_tokenize(str(row['text'])):\n",
    "                sts_list = gensim.utils.simple_preprocess(st)\n",
    "                hist = Counter(sts_list)\n",
    "                for k in hist:\n",
    "                    counts_general[0] += 1\n",
    "                    counts_cnn[0] += 1\n",
    "                    counts_cnn[k] = hist[k] + counts_cnn.get(k, 0)\n",
    "                    counts_general[k] = hist[k] + counts_general.get(k, 0)\n",
    "                sentences_nypost.append(sts_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "counts = dict()\n",
    "counts[0] = 0\n",
    "for sts in sentences_nypost + sentences_cnn:\n",
    "    hist = Counter(sts)\n",
    "    for k in hist:\n",
    "        counts[0] += 1\n",
    "        counts[k] = hist[k] + counts.get(k, 0)\n",
    "json.dump(counts, open(os.path.join('./models', 'general_word_freq_add.json'), 'w'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "json.dump(counts_general, open(os.path.join('./models', 'general_word_freq_add.json'), 'w'))\n",
    "json.dump(counts_nypost, open(os.path.join('./models', 'nypost_word_freq_add.json'), 'w'))\n",
    "json.dump(counts_cnn, open(os.path.join('./models', 'cnn_word_freq_add.json'), 'w'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "opts = {'dims': 100,\n",
    "        'window': 5,\n",
    "        'n_cpu': -1,\n",
    "        'min_count': 30,\n",
    "        'vocab_size': 10000,\n",
    "        'sample': 0.0001,\n",
    "        'n_iter': 30\n",
    "        }\n",
    "model = gensim.models.Word2Vec(sentences=sentences_cnn + sentences_nypost,\n",
    "                               vector_size=opts['dims'],\n",
    "                               window=opts['window'],\n",
    "                               workers=opts['n_cpu'],\n",
    "                               sg=1,\n",
    "                               hs=0,\n",
    "                               negative=5,\n",
    "                               min_count=opts['min_count'],\n",
    "                               max_final_vocab=opts['vocab_size'],\n",
    "                               sample=opts['sample'],\n",
    "                               epochs=opts['n_iter']\n",
    "                               )\n",
    "model.save(os.path.join('./models', 'general_add.model'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "opts = {'dims': 100,\n",
    "        'window': 5,\n",
    "        'n_cpu': -1,\n",
    "        'min_count': 30,\n",
    "        'vocab_size': 10000,\n",
    "        'sample': 0.0001,\n",
    "        'n_iter': 30\n",
    "        }\n",
    "model = gensim.models.Word2Vec(sentences=sentences_nypost,\n",
    "                               vector_size=opts['dims'],\n",
    "                               window=opts['window'],\n",
    "                               workers=opts['n_cpu'],\n",
    "                               sg=1,\n",
    "                               hs=0,\n",
    "                               negative=5,\n",
    "                               min_count=opts['min_count'],\n",
    "                               max_final_vocab=opts['vocab_size'],\n",
    "                               sample=opts['sample'],\n",
    "                               epochs=opts['n_iter']\n",
    "                               )\n",
    "\n",
    "model.save(os.path.join('./models', 'nypost_add.model'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "opts = {'dims': 100,\n",
    "        'window': 5,\n",
    "        'n_cpu': -1,\n",
    "        'min_count': 30,\n",
    "        'vocab_size': 10000,\n",
    "        'sample': 0.0001,\n",
    "        'n_iter': 30\n",
    "        }\n",
    "model = gensim.models.Word2Vec(sentences=sentences_cnn,\n",
    "                               vector_size=opts['dims'],\n",
    "                               window=opts['window'],\n",
    "                               workers=opts['n_cpu'],\n",
    "                               sg=1,\n",
    "                               hs=0,\n",
    "                               negative=5,\n",
    "                               min_count=opts['min_count'],\n",
    "                               max_final_vocab=opts['vocab_size'],\n",
    "                               sample=opts['sample'],\n",
    "                               epochs=opts['n_iter']\n",
    "                               )\n",
    "\n",
    "model.save(os.path.join('./data', 'models', 'cnn_add.model'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from abc import ABC\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stopWords_nltk = set(stopwords.words('english'))\n",
    "stopWords_spc = {'those', 'on', 'own', '’ve', 'yourselves', 'around', 'between', 'four', 'been', 'alone', 'off', 'am',\n",
    "                 'then', 'other', 'can', 'regarding', 'hereafter', 'front', 'too', 'used', 'wherein', '‘ll', 'doing',\n",
    "                 'everything', 'up', 'onto', 'never', 'either', 'how', 'before', 'anyway', 'since', 'through', 'amount',\n",
    "                 'now', 'he', 'was', 'have', 'into', 'because', 'not', 'therefore', 'they', 'n’t', 'even', 'whom', 'it',\n",
    "                 'see', 'somewhere', 'thereupon', 'nothing', 'whereas', 'much', 'whenever', 'seem', 'until', 'whereby',\n",
    "                 'at', 'also', 'some', 'last', 'than', 'get', 'already', 'our', 'once', 'will', 'noone', \"'m\", 'that',\n",
    "                 'what', 'thus', 'no', 'myself', 'out', 'next', 'whatever', 'although', 'though', 'which', 'would',\n",
    "                 'therein', 'nor', 'somehow', 'whereupon', 'besides', 'whoever', 'ourselves', 'few', 'did', 'without',\n",
    "                 'third', 'anything', 'twelve', 'against', 'while', 'twenty', 'if', 'however', 'herself', 'when', 'may',\n",
    "                 'ours', 'six', 'done', 'seems', 'else', 'call', 'perhaps', 'had', 'nevertheless', 'where', 'otherwise',\n",
    "                 'still', 'within', 'its', 'for', 'together', 'elsewhere', 'throughout', 'of', 'others', 'show', '’s',\n",
    "                 'anywhere', 'anyhow', 'as', 'are', 'the', 'hence', 'something', 'hereby', 'nowhere', 'latterly', 'say',\n",
    "                 'does', 'neither', 'his', 'go', 'forty', 'put', 'their', 'by', 'namely', 'could', 'five', 'unless',\n",
    "                 'itself', 'is', 'nine', 'whereafter', 'down', 'bottom', 'thereby', 'such', 'both', 'she', 'become',\n",
    "                 'whole', 'who', 'yourself', 'every', 'thru', 'except', 'very', 'several', 'among', 'being', 'be',\n",
    "                 'mine', 'further', 'n‘t', 'here', 'during', 'why', 'with', 'just', \"'s\", 'becomes', '’ll', 'about',\n",
    "                 'a', 'using', 'seeming', \"'d\", \"'ll\", \"'re\", 'due', 'wherever', 'beforehand', 'fifty', 'becoming',\n",
    "                 'might', 'amongst', 'my', 'empty', 'thence', 'thereafter', 'almost', 'least', 'someone', 'often',\n",
    "                 'from', 'keep', 'him', 'or', '‘m', 'top', 'her', 'nobody', 'sometime', 'across', '‘s', '’re',\n",
    "                 'hundred', 'only', 'via', 'name', 'eight', 'three', 'back', 'to', 'all', 'became', 'move', 'me', 'we',\n",
    "                 'formerly', 'so', 'i', 'whence', 'under', 'always', 'himself', 'in', 'herein', 'more', 'after',\n",
    "                 'themselves', 'you', 'above', 'sixty', 'them', 'your', 'made', 'indeed', 'most', 'everywhere',\n",
    "                 'fifteen', 'but', 'must', 'along', 'beside', 'hers', 'side', 'former', 'anyone', 'full', 'has',\n",
    "                 'yours', 'whose', 'behind', 'please', 'ten', 'seemed', 'sometimes', 'should', 'over', 'take', 'each',\n",
    "                 'same', 'rather', 'really', 'latter', 'and', 'ca', 'hereupon', 'part', 'per', 'eleven', 'ever', '‘re',\n",
    "                 'enough', \"n't\", 'again', '‘d', 'us', 'yet', 'moreover', 'mostly', 'one', 'meanwhile', 'whither',\n",
    "                 'there', 'toward', '’m', \"'ve\", '’d', 'give', 'do', 'an', 'quite', 'these', 'everyone', 'towards',\n",
    "                 'this', 'cannot', 'afterwards', 'beyond', 'make', 'were', 'whether', 'well', 'another', 'below',\n",
    "                 'first', 'upon', 'any', 'none', 'many', 'serious', 'various', 're', 'two', 'less', '‘ve'}\n",
    "stopWords_s = stopWords_spc | stopWords_nltk | STOPWORDS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_general = gensim.models.Word2Vec.load(os.path.join('./models', 'general_add.model'))\n",
    "\n",
    "vocab = list(sorted(list(model_general.wv.index_to_key)))\n",
    "vocab = [w for w in vocab if w not in stopWords_s]\n",
    "mtx = np.vstack([model_general.wv[w] for w in vocab])\n",
    "\n",
    "clustering = KMeans(n_clusters=300).fit(mtx)\n",
    "res = {}\n",
    "t_align = {}\n",
    "for c, w in zip(clustering.labels_, vocab):\n",
    "    t_align[w] = c\n",
    "    c = str(c)\n",
    "    if c not in res:\n",
    "        res[c] = []\n",
    "    res[c].append(w)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "json.dump(res, open(os.path.join('./models', 'news_clustering_add.tpc'), 'w'))\n",
    "\n",
    "t_num = []\n",
    "t_words = []\n",
    "t_name = []\n",
    "for k in res.keys():\n",
    "    t_num.append(k)\n",
    "    t_words.append(res[k])\n",
    "    t_name.append([])\n",
    "pd.DataFrame({'id': t_num, 'name': t_name, 'words': t_words}).to_csv('./news_topics_add.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_general=gensim.models.Word2Vec.load(os.path.join('./models', 'general_add.model'))\n",
    "model_nypost = gensim.models.Word2Vec.load(os.path.join('./models', 'nypost_add.model'))\n",
    "model_cnn = gensim.models.Word2Vec.load(os.path.join('./models', 'cnn_add.model'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shared_vocab = set(model_nypost.wv.key_to_index).intersection(set(model_cnn.wv.key_to_index)).intersection(set(model_general.wv.key_to_index))\n",
    "stopWords = list(stopWords_s & shared_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Aligner(ABC):\n",
    "    def __init__(self, method, source, target, w2id, id2w, mtxA, mtxB, trainvoc):\n",
    "        self.method = method\n",
    "        self.src = source\n",
    "        self.tgt = target\n",
    "        self.w2idA = w2id\n",
    "        self.id2wB = id2w\n",
    "        self.mtxA = mtxA\n",
    "        self.mtxB = mtxB\n",
    "        self.anchors = trainvoc\n",
    "\n",
    "    def translate_mtx(self, mtx):\n",
    "        \"\"\"\n",
    "        MTX -> MTX\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def encode_input(self, words):\n",
    "        \"\"\"\n",
    "        [STRING] -> MTX\n",
    "        \"\"\"\n",
    "        embs = [self.mtxA[self.w2idA[w], :] for w in words]\n",
    "        return np.vstack(embs)\n",
    "\n",
    "    def decode_output(self, mtx, k=1):\n",
    "        \"\"\"\n",
    "        MTX -> [[STRING]]\n",
    "        \"\"\"\n",
    "        similarities = cosine_similarity(mtx, self.mtxB)\n",
    "        most_similar = np.argsort(similarities, axis=1)[:, ::-1]\n",
    "        topsims = np.sort(similarities, axis=1)[:, ::-1][:, :k]\n",
    "        res = [[self.id2wB[i] for i in row[:k]] for row in most_similar]\n",
    "        return res, topsims\n",
    "\n",
    "    def translate_word(self, word, k=1):\n",
    "        \"\"\"\n",
    "        STRING -> STRING\n",
    "        \"\"\"\n",
    "        encoding = self.encode_input([word])\n",
    "        translated = self.translate_mtx(encoding)\n",
    "        decoded = self.decode_output(translated, k=k)\n",
    "        return decoded[0][:k]\n",
    "\n",
    "    def translate_words(self, words, k=1):\n",
    "        \"\"\"\n",
    "        [STRING] -> [STRING]\n",
    "        \"\"\"\n",
    "        encoding = self.encode_input(words)\n",
    "        translated = self.translate_mtx(encoding)\n",
    "        decoded, simscores = self.decode_output(translated, k=k)\n",
    "        return decoded, simscores\n",
    "\n",
    "\n",
    "class CCAAligner(Aligner):\n",
    "    def set_params(self, cca):\n",
    "        self.cca = cca\n",
    "\n",
    "    def translate_mtx(self, mtx):\n",
    "        return mtx\n",
    "\n",
    "    def translate_word(self, word, k=1):\n",
    "        tmpA = self.mtxA\n",
    "        tmpB = self.mtxB\n",
    "        self.mtxA, self.mtxB = self.cca.transform(tmpA, tmpB)\n",
    "        res = super().translate_word(word, k=k)\n",
    "        self.mtxA = tmpA\n",
    "        self.mtxB = tmpB\n",
    "        return res\n",
    "\n",
    "    def translate_words(self, words, k=1):\n",
    "        tmpA = self.mtxA\n",
    "        tmpB = self.mtxB\n",
    "        self.mtxA, self.mtxB = self.cca.transform(tmpA, tmpB)\n",
    "        res, simscores = super().translate_words(words, k=k)\n",
    "        self.mtxA = tmpA\n",
    "        self.mtxB = tmpB\n",
    "        return res, simscores\n",
    "\n",
    "\n",
    "class SVDAligner(Aligner):\n",
    "    def set_params(self, T):\n",
    "        self.T = T\n",
    "\n",
    "    def translate_mtx(self, mtx):\n",
    "        return mtx.dot(self.T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def align_cca(source, target):\n",
    "    N_dims = source.shape[1]\n",
    "    cca = CCA(n_components=N_dims, max_iter=2000)\n",
    "    cca.fit(source, target)\n",
    "    return cca\n",
    "\n",
    "\n",
    "def align_svd(source, target):\n",
    "    product = np.matmul(source.transpose(), target)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "    T = np.matmul(U, V)\n",
    "    return T\n",
    "\n",
    "\n",
    "def get_cca_aligner(model_a, model_b, anchorlist):\n",
    "    # get wordmaps\n",
    "    awords = list(sorted(list(model_a.wv.key_to_index)))\n",
    "    bwords = list(sorted(list(model_b.wv.key_to_index)))\n",
    "    w2idA = {w: i for i, w in enumerate(awords)}\n",
    "    id2wA = {i: w for i, w in enumerate(awords)}\n",
    "    w2idB = {w: i for i, w in enumerate(bwords)}\n",
    "    id2wB = {i: w for i, w in enumerate(bwords)}\n",
    "\n",
    "    # build the base matrices\n",
    "    a_mtx = np.vstack([model_a.wv[w] for w in awords])\n",
    "    b_mtx = np.vstack([model_b.wv[w] for w in bwords])\n",
    "\n",
    "    # get the anchors\n",
    "    a_anchor = np.vstack([a_mtx[w2idA[w], :] for w in anchorlist])\n",
    "    b_anchor = np.vstack([b_mtx[w2idB[w], :] for w in anchorlist])\n",
    "\n",
    "    # compute CCA\n",
    "    cca = align_cca(a_anchor, b_anchor)\n",
    "\n",
    "    # build and return the aligner\n",
    "    aligner = CCAAligner('cca', model_a, model_b, w2idA, id2wB, a_mtx, b_mtx, anchorlist)\n",
    "    aligner.set_params(cca)\n",
    "    return aligner\n",
    "\n",
    "\n",
    "def get_svd_aligner(model_a, model_b, anchorlist):\n",
    "    # get wordmaps\n",
    "    awords = list(sorted(list(model_a.wv.vocab)))\n",
    "    bwords = list(sorted(list(model_b.wv.vocab)))\n",
    "    w2idA = {w: i for i, w in enumerate(awords)}\n",
    "    w2idB = {w: i for i, w in enumerate(bwords)}\n",
    "    id2wB = {i: w for i, w in enumerate(bwords)}\n",
    "\n",
    "    # build the base matrices\n",
    "    a_mtx = np.vstack([model_a.wv[w] for w in awords])\n",
    "    b_mtx = np.vstack([model_b.wv[w] for w in bwords])\n",
    "    print(a_mtx.shape, b_mtx.shape)\n",
    "\n",
    "    # get the anchors\n",
    "    a_anchor = np.vstack([a_mtx[w2idA[w], :] for w in anchorlist])\n",
    "    b_anchor = np.vstack([b_mtx[w2idB[w], :] for w in anchorlist])\n",
    "\n",
    "    # get the translation matrix\n",
    "    T = align_svd(a_anchor, b_anchor)\n",
    "\n",
    "    # build and return the aligner\n",
    "    aligner = SVDAligner('svd', model_a, model_b, w2idA, id2wB, a_mtx, b_mtx, anchorlist)\n",
    "    aligner.set_params(T)\n",
    "    return aligner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aligner_cnn = get_cca_aligner(model_cnn, model_general, stopWords)\n",
    "pickle.dump(aligner_cnn, open(os.path.join('./models', 'align_cnn_add.pkl'), 'wb'))\n",
    "aligner_nypost = get_cca_aligner(model_nypost, model_general, stopWords)\n",
    "pickle.dump(aligner_nypost, open(os.path.join('./models', 'align_nypost_add.pkl'), 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forward_cnn = pickle.load(open(os.path.join('./models', 'align_cnn_add.pkl'), 'rb'))\n",
    "forward_nypost = pickle.load(open(os.path.join('./models', 'align_nypost_add.pkl'), 'rb'))\n",
    "check = 'obama'\n",
    "print(forward_cnn.translate_word(check, k=10), forward_nypost.translate_word(check, k=10))\n",
    "ny_map=forward_nypost.translate_mtx(model_nypost.wv[check])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from abc import ABC\n",
    "from collections import Counter\n",
    "\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def JS_divergence(p, q):\n",
    "    M = (p + q) / 2\n",
    "    return 0.5 * scipy.stats.entropy(p, M, base=2) + 0.5 * scipy.stats.entropy(q, M, base=2)\n",
    "\n",
    "def research_topic(keywords):\n",
    "    cnn_path = f\"CNN_{keywords}\"\n",
    "    for f_name in os.listdir(\"./data/CNN\"):\n",
    "        if f_name.startswith(cnn_path):\n",
    "            break\n",
    "    cnn_file = open(os.path.join(\"./data/CNN\", f_name), 'r')\n",
    "\n",
    "    nypost_path = f\"nypost_{keywords}\"\n",
    "    for f_name in os.listdir(\"./data/nypost\"):\n",
    "        if f_name.startswith(nypost_path):\n",
    "            break\n",
    "    nypost_file = open(os.path.join(\"./data/nypost\", f_name), 'r')\n",
    "\n",
    "    word_intersec = []\n",
    "    t_vec_cnn = np.zeros(300)\n",
    "    df = pd.read_csv(cnn_file)\n",
    "    all_cnn = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        for st in sent_tokenize(str(row['text'])):\n",
    "            sts_list = gensim.utils.simple_preprocess(st)\n",
    "            hist = Counter(sts_list)\n",
    "            for k in hist:\n",
    "                num = t_align.get(k, -1)\n",
    "                if num >= 0:\n",
    "                    word_intersec.append(k)\n",
    "                    all_cnn += hist[k]\n",
    "                    t_vec_cnn[num] += hist[k]\n",
    "    t_vec_cnn /= all_cnn\n",
    "\n",
    "    t_vec_nypost = np.zeros(300)\n",
    "    df = pd.read_csv(nypost_file)\n",
    "    all_nypost = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        for st in sent_tokenize(str(row['text'])):\n",
    "            sts_list = gensim.utils.simple_preprocess(st)\n",
    "            hist = Counter(sts_list)\n",
    "            for k in hist:\n",
    "                num = t_align.get(k, -1)\n",
    "                if num >= 0:\n",
    "                    word_intersec.append(k)\n",
    "                    all_nypost += hist[k]\n",
    "                    t_vec_nypost[num] += hist[k]\n",
    "    t_vec_nypost /= all_nypost\n",
    "\n",
    "    topic_js = JS_divergence(t_vec_nypost, t_vec_cnn)\n",
    "    topic_cos = cosine_similarity(np.array(t_vec_nypost).reshape([1, -1]), np.array(t_vec_cnn).reshape([1, -1]))\n",
    "\n",
    "    forward_cnn = pickle.load(open(os.path.join('./models', 'align_cnn_add.pkl'), 'rb'))\n",
    "    forward_nypost = pickle.load(open(os.path.join('./models', 'align_nypost_add.pkl'), 'rb'))\n",
    "\n",
    "    dis = []\n",
    "    for wd in word_intersec:\n",
    "        if wd in model_general.wv.key_to_index and wd in model_cnn.wv.key_to_index:\n",
    "            vec_nypost = forward_nypost.translate_mtx(model_nypost.wv[wd])\n",
    "            vec_cnn = forward_cnn.translate_mtx(model_cnn.wv[wd])\n",
    "            cos_sim = cosine_similarity(np.array(vec_nypost).reshape([1, -1]), np.array(vec_cnn).reshape([1, -1]))\n",
    "            dis.append(cos_sim)\n",
    "    mean_c = np.mean(np.array(dis))\n",
    "\n",
    "    return topic_js, topic_cos, mean_c, t_vec_cnn, t_vec_nypost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topics = [\"attack\", \"Biden\", \"black\", \"China\", \"conflict\", \"crime\", \"democratic\", \"fair\", \"gun\", \"immigration\",\n",
    "          \"invasion\", \"LGBT\", \"police\", \"protest\", \"refused\", \"republic\", \"Russia\", \"terror\", \"Trump\", \"UK\", \"ukraine\",\n",
    "          \"US\", \"violence\", \"war\"]\n",
    "\n",
    "t_js_list = []\n",
    "t_cos_list = []\n",
    "mean_list = []\n",
    "t_cnn = []\n",
    "t_nypost = []\n",
    "\n",
    "for t in topics:\n",
    "    print(t)\n",
    "    a, b, c, d, e = research_topic(t)\n",
    "    t_js_list.append(a)\n",
    "    t_cos_list.append(b)\n",
    "    mean_list.append(c)\n",
    "    t_cnn.append(d)\n",
    "    t_nypost.append(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.ylabel('content similarity by cosine similarity')\n",
    "pd.DataFrame({\n",
    "    'mean_simi': mean_list,\n",
    "    'JS_div': t_js_list,\n",
    "    'Cos_div': t_cos_list,\n",
    "    't_cnn': t_cnn,\n",
    "    't_nypost': t_nypost\n",
    "},\n",
    "    index=topics\n",
    ").to_csv('./records_add.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}