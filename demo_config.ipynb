{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Group ACE project: Difference between left-wing media and right-wing media in the US\n",
    "\n",
    "Code for data collection, cleaning/wrangling, analysis, and visualizations;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Data Collection & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from crawl import SearchEngine\n",
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kargs1 = {'name': 'CNN',                      # name of the news website\n",
    "          'keyword': 'US',                    # keyword to search\n",
    "          'startpage': 1,\n",
    "          'endpage': 20,\n",
    "          'sleep1': 1,                        # time interval to revisit the page if failed\n",
    "          'sleep2': 1,                        # time interval to visit the next page if current page is empty\n",
    "          'sleep3': 1,                        # time interval to reparse the page if failed\n",
    "          'limit1': 10,                       # number of times to revisit the page if failed, then skip the page\n",
    "          'limit2': 10,                       # number of times to visit next page if current page is empty, then end the crawler\n",
    "          'limit3': 3,                        # total number of times to reparse the page if failed, then end the crawler\n",
    "          'filter_': None,                    # time filter for the news\n",
    "          'process': False,                   # whether to get the urls in debug\n",
    "          'parse': False,                     # whether to parse the urls in debug\n",
    "          'save': False,                      # whether to save the news in debug\n",
    "          'root': f'../data/test',}                   # root directory to save the news\n",
    "keywords = open('./crawler//keywords.txt', 'r').read().split('\\n')\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "search = SearchEngine(**kargs1)\n",
    "search.get_urls(1)\n",
    "search.get_all_urls()\n",
    "search.parse()\n",
    "search.auto(['CNN', 'nypost'] , keywords[:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Embedding Model Training\n",
    "This part is a little bit complex, I will explain it more in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Gather the texts collected above, convert them into trainable corpus.\n",
    "# At the same time, calculate their word frequency for further use.\n",
    "sentences_cnn = []\n",
    "sentences_nypost = []\n",
    "\n",
    "counts_general = dict()\n",
    "counts_cnn = dict()\n",
    "counts_nypost = dict()\n",
    "counts_general[0] = 0\n",
    "counts_cnn[0] = 0\n",
    "counts_nypost[0] = 0\n",
    "for fname in os.listdir('./data/nypost/'):\n",
    "    if len(fname) > 12:\n",
    "        print(fname)\n",
    "        df = pd.read_csv(os.path.join('./data/nypost', fname))\n",
    "        for idx, row in df.iterrows():\n",
    "            for st in sent_tokenize(str(row['text'])):\n",
    "                sts_list = gensim.utils.simple_preprocess(st)\n",
    "                hist = Counter(sts_list)\n",
    "                for k in hist:\n",
    "                    counts_general[0] += 1\n",
    "                    counts_nypost[0] += 1\n",
    "                    counts_general[k] = hist[k] + counts_general.get(k, 0)\n",
    "                    counts_nypost[k] = hist[k] + counts_general.get(k, 0)\n",
    "                sentences_cnn.append(sts_list)\n",
    "\n",
    "\n",
    "for fname in os.listdir('./data/CNN/'):\n",
    "    if len(fname) > 12:\n",
    "        print(fname)\n",
    "        df = pd.read_csv(os.path.join('./data/CNN', fname))\n",
    "        for idx, row in df.iterrows():\n",
    "            for st in sent_tokenize(str(row['text'])):\n",
    "                sts_list = gensim.utils.simple_preprocess(st)\n",
    "                hist = Counter(sts_list)\n",
    "                for k in hist:\n",
    "                    counts_general[0] += 1\n",
    "                    counts_cnn[0] += 1\n",
    "                    counts_cnn[k] = hist[k] + counts_cnn.get(k, 0)\n",
    "                    counts_general[k] = hist[k] + counts_general.get(k, 0)\n",
    "                sentences_nypost.append(sts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "json.dump(counts_general, open(os.path.join('./wv', 'general_word_freq_add.json'), 'w'))\n",
    "json.dump(counts_nypost, open(os.path.join('./wv', 'nypost_word_freq_add.json'), 'w'))\n",
    "json.dump(counts_cnn, open(os.path.join('./wv', 'cnn_word_freq_add.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train the general model based on combined corpus.\n",
    "opts = {'dims': 100,\n",
    "        'window': 5,\n",
    "        'n_cpu': -1,\n",
    "        'min_count': 30,\n",
    "        'vocab_size': 10000,\n",
    "        'sample': 0.0001,\n",
    "        'n_iter': 30\n",
    "        }\n",
    "model = gensim.models.Word2Vec(sentences=sentences_cnn + sentences_nypost,\n",
    "                               vector_size=opts['dims'],\n",
    "                               window=opts['window'],\n",
    "                               workers=opts['n_cpu'],\n",
    "                               sg=1,\n",
    "                               hs=0,\n",
    "                               negative=5,\n",
    "                               min_count=opts['min_count'],\n",
    "                               max_final_vocab=opts['vocab_size'],\n",
    "                               sample=opts['sample'],\n",
    "                               epochs=opts['n_iter']\n",
    "                               )\n",
    "model.save(os.path.join('./wv', 'general_add.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train the W2V model for republic based on nypost.\n",
    "opts = {'dims': 100,\n",
    "        'window': 5,\n",
    "        'n_cpu': -1,\n",
    "        'min_count': 30,\n",
    "        'vocab_size': 10000,\n",
    "        'sample': 0.0001,\n",
    "        'n_iter': 30\n",
    "        }\n",
    "model = gensim.models.Word2Vec(sentences=sentences_nypost,\n",
    "                               vector_size=opts['dims'],\n",
    "                               window=opts['window'],\n",
    "                               workers=opts['n_cpu'],\n",
    "                               sg=1,\n",
    "                               hs=0,\n",
    "                               negative=5,\n",
    "                               min_count=opts['min_count'],\n",
    "                               max_final_vocab=opts['vocab_size'],\n",
    "                               sample=opts['sample'],\n",
    "                               epochs=opts['n_iter']\n",
    "                               )\n",
    "\n",
    "model.save(os.path.join('./wv', 'nypost_add.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model for left wing based on cnn model.\n",
    "opts = {'dims': 100,\n",
    "        'window': 5,\n",
    "        'n_cpu': -1,\n",
    "        'min_count': 30,\n",
    "        'vocab_size': 10000,\n",
    "        'sample': 0.0001,\n",
    "        'n_iter': 30\n",
    "        }\n",
    "model = gensim.models.Word2Vec(sentences=sentences_cnn,\n",
    "                               vector_size=opts['dims'],\n",
    "                               window=opts['window'],\n",
    "                               workers=opts['n_cpu'],\n",
    "                               sg=1,\n",
    "                               hs=0,\n",
    "                               negative=5,\n",
    "                               min_count=opts['min_count'],\n",
    "                               max_final_vocab=opts['vocab_size'],\n",
    "                               sample=opts['sample'],\n",
    "                               epochs=opts['n_iter']\n",
    "                               )\n",
    "\n",
    "model.save(os.path.join('./wv', 'cnn_add.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Model Aligning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from align import CCAAligner, get_cca_aligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stopWords_nltk = set(stopwords.words('english'))\n",
    "stopWords_spc = {'those', 'on', 'own', '’ve', 'yourselves', 'around', 'between', 'four', 'been', 'alone', 'off', 'am',\n",
    "                 'then', 'other', 'can', 'regarding', 'hereafter', 'front', 'too', 'used', 'wherein', '‘ll', 'doing',\n",
    "                 'everything', 'up', 'onto', 'never', 'either', 'how', 'before', 'anyway', 'since', 'through', 'amount',\n",
    "                 'now', 'he', 'was', 'have', 'into', 'because', 'not', 'therefore', 'they', 'n’t', 'even', 'whom', 'it',\n",
    "                 'see', 'somewhere', 'thereupon', 'nothing', 'whereas', 'much', 'whenever', 'seem', 'until', 'whereby',\n",
    "                 'at', 'also', 'some', 'last', 'than', 'get', 'already', 'our', 'once', 'will', 'noone', \"'m\", 'that',\n",
    "                 'what', 'thus', 'no', 'myself', 'out', 'next', 'whatever', 'although', 'though', 'which', 'would',\n",
    "                 'therein', 'nor', 'somehow', 'whereupon', 'besides', 'whoever', 'ourselves', 'few', 'did', 'without',\n",
    "                 'third', 'anything', 'twelve', 'against', 'while', 'twenty', 'if', 'however', 'herself', 'when', 'may',\n",
    "                 'ours', 'six', 'done', 'seems', 'else', 'call', 'perhaps', 'had', 'nevertheless', 'where', 'otherwise',\n",
    "                 'still', 'within', 'its', 'for', 'together', 'elsewhere', 'throughout', 'of', 'others', 'show', '’s',\n",
    "                 'anywhere', 'anyhow', 'as', 'are', 'the', 'hence', 'something', 'hereby', 'nowhere', 'latterly', 'say',\n",
    "                 'does', 'neither', 'his', 'go', 'forty', 'put', 'their', 'by', 'namely', 'could', 'five', 'unless',\n",
    "                 'itself', 'is', 'nine', 'whereafter', 'down', 'bottom', 'thereby', 'such', 'both', 'she', 'become',\n",
    "                 'whole', 'who', 'yourself', 'every', 'thru', 'except', 'very', 'several', 'among', 'being', 'be',\n",
    "                 'mine', 'further', 'n‘t', 'here', 'during', 'why', 'with', 'just', \"'s\", 'becomes', '’ll', 'about',\n",
    "                 'a', 'using', 'seeming', \"'d\", \"'ll\", \"'re\", 'due', 'wherever', 'beforehand', 'fifty', 'becoming',\n",
    "                 'might', 'amongst', 'my', 'empty', 'thence', 'thereafter', 'almost', 'least', 'someone', 'often',\n",
    "                 'from', 'keep', 'him', 'or', '‘m', 'top', 'her', 'nobody', 'sometime', 'across', '‘s', '’re',\n",
    "                 'hundred', 'only', 'via', 'name', 'eight', 'three', 'back', 'to', 'all', 'became', 'move', 'me', 'we',\n",
    "                 'formerly', 'so', 'i', 'whence', 'under', 'always', 'himself', 'in', 'herein', 'more', 'after',\n",
    "                 'themselves', 'you', 'above', 'sixty', 'them', 'your', 'made', 'indeed', 'most', 'everywhere',\n",
    "                 'fifteen', 'but', 'must', 'along', 'beside', 'hers', 'side', 'former', 'anyone', 'full', 'has',\n",
    "                 'yours', 'whose', 'behind', 'please', 'ten', 'seemed', 'sometimes', 'should', 'over', 'take', 'each',\n",
    "                 'same', 'rather', 'really', 'latter', 'and', 'ca', 'hereupon', 'part', 'per', 'eleven', 'ever', '‘re',\n",
    "                 'enough', \"n't\", 'again', '‘d', 'us', 'yet', 'moreover', 'mostly', 'one', 'meanwhile', 'whither',\n",
    "                 'there', 'toward', '’m', \"'ve\", '’d', 'give', 'do', 'an', 'quite', 'these', 'everyone', 'towards',\n",
    "                 'this', 'cannot', 'afterwards', 'beyond', 'make', 'were', 'whether', 'well', 'another', 'below',\n",
    "                 'first', 'upon', 'any', 'none', 'many', 'serious', 'various', 're', 'two', 'less', '‘ve'}\n",
    "stopWords_s = stopWords_spc | stopWords_nltk | STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This part is to do content cluster. I put all the words into 300 clusters based on their vector by general W2V model. So that I can get what are the general content units in newspaper discourse.\n",
    "# Later, I can use there content clusters to represent articles content coverage by calculating their percentage of words of each cluster.\n",
    "model_general = gensim.models.Word2Vec.load(os.path.join('./models', 'general_add.model'))\n",
    "\n",
    "vocab = list(sorted(list(model_general.wv.index_to_key)))\n",
    "vocab = [w for w in vocab if w not in stopWords_s]\n",
    "mtx = np.vstack([model_general.wv[w] for w in vocab])\n",
    "\n",
    "clustering = KMeans(n_clusters=300).fit(mtx)\n",
    "res = {}\n",
    "t_align = {}\n",
    "for c, w in zip(clustering.labels_, vocab):\n",
    "    t_align[w] = c\n",
    "    c = str(c)\n",
    "    if c not in res:\n",
    "        res[c] = []\n",
    "    res[c].append(w)\n",
    "json.dump(res, open(os.path.join('./models', 'news_clustering_add.tpc'), 'w'))\n",
    "pickle.dump(t_align, open(os.path.join('./models', 't_align.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "c_num = []\n",
    "c_words = []\n",
    "c_name = []\n",
    "for k in res.keys():\n",
    "    c_num.append(k)\n",
    "    c_words.append(res[k])\n",
    "    c_name.append([])\n",
    "pd.DataFrame({'id': c_num, 'name': c_name, 'words': c_words}).to_csv('./news_topics_add.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_general=gensim.models.Word2Vec.load(os.path.join('./models', 'general_add.model'))\n",
    "model_nypost = gensim.models.Word2Vec.load(os.path.join('./models', 'nypost_add.model'))\n",
    "model_cnn = gensim.models.Word2Vec.load(os.path.join('./models', 'cnn_add.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the anchor words by intersection of shared words and stopwords.\n",
    "# The intuition is that, whatever the political polarization is, the meaning of stopwords should be the same. Words as plotted below are examples.\n",
    "shared_vocab = set(model_nypost.wv.key_to_index).intersection(set(model_cnn.wv.key_to_index)).intersection(set(model_general.wv.key_to_index))\n",
    "stopWords = list(stopWords_s & shared_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Then, I train the cross-lingual translation model. Its like translate concepts between polarized language and neutral language. By using a neutral set of language to explain and represent polarized language, I can measure the semantic difference between two polarized ideological discourse.\n",
    "aligner_cnn = get_cca_aligner(model_cnn, model_general, stopWords)\n",
    "pickle.dump(aligner_cnn, open(os.path.join('./models', 'align_cnn_add.pkl'), 'wb'))\n",
    "aligner_nypost = get_cca_aligner(model_nypost, model_general, stopWords)\n",
    "pickle.dump(aligner_nypost, open(os.path.join('./models', 'align_nypost_add.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Analysis: Topic Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1. Word Cloud\n",
    "- For each topic and media, we generated a word cloud to represent words’ relative importances. This provides an initial impression of the differences between left-wing and right-wing media in representing the same topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from analysis import Word_Cloud\n",
    "kargs2 = {'medias': ['CNN', 'nypost'],                  # medias to include\n",
    "          'keywords': keywords,                         # keywords to include\n",
    "          'root': '../data',                               # root directory to load the news\n",
    "          'limit': 1000,                                # number of news to load\n",
    "          'custom_stopwords':open('./crawler/custom_stopwords.txt', 'r').read().split('\\n'),}  # add custom stopwords\n",
    "kargs2['custom_stopwords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wc = Word_Cloud(**kargs2)\n",
    "wc.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2. Word Cluster Analysis (Topic Modeling)\n",
    "- Gensim Latent Dirichlet Allocation (LDA) to topic model for each keyword (aka topic) with generated articles from CNN and New York Post to learn the topic distributions and word distributions that best explain these collection of articles\n",
    "- further compare left-wing and right-wing medias’ content differences. \n",
    "- For each topic, we model five topic clusters using searched articles. Then, we visualize the topic clusters of each topic for each media (CNN and New York Post). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gensim import models,corpora\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from analysis import get_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get dataframe composed of all csvs in CNN folder\n",
    "csvs = [x for x in os.listdir('./data/CNN/') if x.endswith('.csv')]\n",
    "fns = [os.path.splitext(os.path.basename(x))[0] for x in csvs]\n",
    "topics=[]\n",
    "for i in fns:\n",
    "    topics.append(i.split('_')[1])\n",
    "d = {}\n",
    "for i in range(len(fns)):\n",
    "    d[topics[i]] = pd.read_csv(os.path.join('./data/CNN',csvs[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get pyLDAvis visualizations for all topic models under CNN\n",
    "cnn_topics=[]\n",
    "ps = []\n",
    "for i in fns:\n",
    "    cnn_topics.append(i.split('_')[1])\n",
    "\n",
    "for i in cnn_topics:\n",
    "    model = models.ldamodel.LdaModel.load('./topicmodeling/models/{}CNN.model'.format(i))\n",
    "    topics = model.print_topics(num_words=20)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "    lemmas = d[i]['text'].apply(get_lemmas)\n",
    "    dictionary = corpora.Dictionary.load('./topicmodeling/models/{}CNN.model.id2word'.format(i))\n",
    "    corpus = [dictionary.doc2bow(text) for text in lemmas]\n",
    "    pyLDAvis.enable_notebook()\n",
    "    p = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
    "    ps.append(p)\n",
    "    print(i+'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import nypost data\n",
    "ny_csvs = [x for x in os.listdir('./data/nypost/') if x.endswith('.csv')]\n",
    "fns = [os.path.splitext(os.path.basename(x))[0] for x in ny_csvs]\n",
    "ny_topics=[]\n",
    "for i in fns:\n",
    "    ny_topics.append(i.split('_')[1])\n",
    "ny_d = {}\n",
    "for i in range(len(fns)):\n",
    "    ny_d[ny_topics[i]] = pd.read_csv(os.path.join('./data/nypost',ny_csvs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get pyLDAvis visualizations for all topic models under nypost\n",
    "ny_topics=[]\n",
    "ps_ny = []\n",
    "for i in fns:\n",
    "    ny_topics.append(i.split('_')[1])\n",
    "for i in cnn_topics:\n",
    "    model = models.ldamodel.LdaModel.load('./topicmodeling/models/{}nypost.model'.format(i))\n",
    "    topics = model.print_topics(num_words=20)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "    lemmas = d[i]['text'].apply(get_lemmas)\n",
    "    dictionary = corpora.Dictionary.load('./topicmodeling/models/{}nypost.model.id2word'.format(i))\n",
    "    corpus = [dictionary.doc2bow(text) for text in lemmas]\n",
    "    pyLDAvis.enable_notebook()\n",
    "    p = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
    "    ps_ny.append(p)\n",
    "    print(i+'done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Analysis: Content Coverage & Ideological Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "forward_cnn = pickle.load(open(os.path.join('./models', 'align_cnn_add.pkl'), 'rb'))\n",
    "forward_nypost = pickle.load(open(os.path.join('./models', 'align_nypost_add.pkl'), 'rb'))\n",
    "t_align=pickle.load(open(os.path.join('./models', 't_align.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from align import research_topic\n",
    "\n",
    "topics = [\"attack\", \"Biden\", \"black\", \"China\", \"conflict\", \"crime\", \"democratic\", \"fair\", \"gun\", \"immigration\",\n",
    "          \"invasion\", \"LGBT\", \"police\", \"protest\", \"refused\", \"republic\", \"Russia\", \"terror\", \"Trump\", \"UK\", \"ukraine\",\n",
    "          \"US\", \"violence\", \"war\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "t_js_list = []\n",
    "t_cos_list = []\n",
    "mean_list = []\n",
    "t_cnn = []\n",
    "t_nypost = []\n",
    "\n",
    "for t in topics:\n",
    "    print(t)\n",
    "    a, b, c, d, e = research_topic(t,t_align,forward_cnn,forward_nypost,model_general,model_cnn,model_nypost)\n",
    "    t_js_list.append(a)\n",
    "    t_cos_list.append(b)\n",
    "    mean_list.append(c)\n",
    "    t_cnn.append(d)\n",
    "    t_nypost.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'mean_simi': mean_list,\n",
    "    'JS_div': t_js_list,\n",
    "    'Cos_div': t_cos_list,\n",
    "    't_cnn': t_cnn,\n",
    "    't_nypost': t_nypost\n",
    "},\n",
    "    index=topics\n",
    ").to_csv('./records_add.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "other = [1, 2, 3, 6, 7, 9, 11, 15, 16, 18, 19, 21]\n",
    "war = [10, 17, 20, 23]\n",
    "violence = [0, 4, 5, 8, 12, 13, 14, 22]\n",
    "\n",
    "df = pd.read_csv('./records.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 10])\n",
    "plt.scatter(np.array(df.iloc[other]['JS_div']), -np.array(df.iloc[other]['mean_simi']), c='b', label='other topics')\n",
    "plt.scatter(np.array(df.iloc[violence]['JS_div']), -np.array(df.iloc[violence]['mean_simi']), c='y',\n",
    "            label='violence topics')\n",
    "plt.scatter(np.array(df.iloc[war]['JS_div']), -np.array(df.iloc[war]['mean_simi']), c='r', label='war topics')\n",
    "plt.legend()\n",
    "for i in range(len(topics)):\n",
    "    plt.annotate(topics[i], (df['JS_div'][i], -df['mean_simi'][i]))\n",
    "plt.xlabel('Content coverage diversity by JS divergence')\n",
    "plt.ylabel('Ideological context diversity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
