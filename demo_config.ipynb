{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Group ACE project: Difference between left-wing media and right-wing media in the US\n",
    "\n",
    "Code for data collection, cleaning/wrangling, analysis, and visualizations;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Data Collection & Cleaning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from crawl import SearchEngine\n",
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kargs1 = {'name': 'CNN',                      # name of the news website\n",
    "          'keyword': 'US',                    # keyword to search\n",
    "          'startpage': 1,\n",
    "          'endpage': 20,\n",
    "          'sleep1': 1,                        # time interval to revisit the page if failed\n",
    "          'sleep2': 1,                        # time interval to visit the next page if current page is empty\n",
    "          'sleep3': 1,                        # time interval to reparse the page if failed\n",
    "          'limit1': 10,                       # number of times to revisit the page if failed, then skip the page\n",
    "          'limit2': 10,                       # number of times to visit next page if current page is empty, then end the crawler\n",
    "          'limit3': 3,                        # total number of times to reparse the page if failed, then end the crawler\n",
    "          'filter_': None,                    # time filter for the news\n",
    "          'process': False,                   # whether to get the urls in debug\n",
    "          'parse': False,                     # whether to parse the urls in debug\n",
    "          'save': False,                      # whether to save the news in debug\n",
    "          'root': f'../data/test',}                   # root directory to save the news\n",
    "keywords = open('keywords.txt', 'r').read().split('\\n')\n",
    "print(keywords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "search = SearchEngine(**kargs1)\n",
    "search.get_urls(1)\n",
    "search.get_all_urls()\n",
    "search.parse()\n",
    "search.auto(['CNN', 'nypost'] , keywords[:2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Embedding Model Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lyu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nypost_gun_page1to10000_timeto.csv\n",
      "nypost_Trump_page1to10000_timeto.csv\n",
      "nypost_Biden_page1to10000_timeto.csv\n",
      "nypost_invasion_page1to5000_timeto.csv\n",
      "nypost_republic_page1to10000_timeto.csv\n",
      "nypost_Biden.csv\n",
      "nypost_immigration.csv\n",
      "nypost_terror_page1to10000_timeto.csv\n",
      "nypost_ukraine_page1to10000_timeto.csv\n",
      "nypost_crime.csv\n",
      "nypost_UK.csv\n",
      "nypost_Trump.csv\n",
      "nypost_Russia.csv\n",
      "nypost_conflict.csv\n",
      "nypost_crime_page1to5000_timeto.csv\n",
      "nypost_US_page1to10000_timeto.csv\n",
      "nypost_attack_page1to10000_timeto.csv\n",
      "nypost_violence.csv\n",
      "nypost_violence_page1to10000_timeto.csv\n",
      "nypost_terror.csv\n",
      "nypost_Russia_page1to10000_timeto.csv\n",
      "nypost_refused.csv\n",
      "nypost_democratic_page1to10000_timeto.csv\n",
      "nypost_LGBT_page1to10000_timeto.csv\n",
      "nypost_immigration_page1to10000_timeto.csv\n",
      "nypost_fair.csv\n",
      "nypost_protest.csv\n",
      "nypost_invasion.csv\n",
      "nypost_fair_page1to10000_timeto.csv\n",
      "nypost_UK_page1to10000_timeto.csv\n",
      "nypost_democratic.csv\n",
      "nypost_US.csv\n",
      "nypost_LGBT.csv\n",
      "nypost_China.csv\n",
      "nypost_police_page1to10000_timeto.csv\n",
      "nypost_police.csv\n",
      "nypost_gun.csv\n",
      "nypost_black_page1to10000_timeto.csv\n",
      "nypost_ukraine.csv\n",
      "nypost_war_page1to5000_timeto.csv\n",
      "nypost_black.csv\n",
      "nypost_protest_page1to10000_timeto.csv\n",
      "nypost_refused_page1to5000_timeto.csv\n",
      "nypost_war.csv\n",
      "nypost_conflict__page1to10000_timeto.csv\n",
      "nypost_China_page1to10000_timeto.csv\n",
      "nypost_attack.csv\n",
      "nypost_republic.csv\n",
      "CNN_war_page1to5000_timeto.csv\n",
      "CNN_ukraine.csv\n",
      "CNN_Trump.csv\n",
      "CNN_crime.csv\n",
      "CNN_UK_page1to1000_timeto.csv\n",
      "CNN_police.csv\n",
      "CNN_republic_page1to5000_timeto.csv\n",
      "CNN_invasion.csv\n",
      "CNN_attack.csv\n",
      "CNN_republic.csv\n",
      "CNN_crime_page1to5000_timeto.csv\n",
      "CNN_gun_page1to5000_timeto.csv\n",
      "CNN_protest.csv\n",
      "CNN_LGBT_page1to5000_timeto.csv\n",
      "CNN_Biden.csv\n",
      "CNN_violence_page1to5000_timeto.csv\n",
      "CNN_Biden_page1to1000_timeto.csv\n",
      "CNN_democratic_page1to5000_timeto.csv\n",
      "CNN_refused_page1to5000_timeto.csv\n",
      "CNN_fair_page1to5000_timeto.csv\n",
      "CNN_protest_page1to5000_timeto.csv\n",
      "CNN_attack_page1to5000_timeto.csv\n",
      "CNN_black.csv\n",
      "CNN_Trump_page1to5000_timeto.csv\n",
      "CNN_ukraine_page1to5000_timeto.csv\n",
      "CNN_refused.csv\n",
      "CNN_terror.csv\n",
      "CNN_black_page1to5000_timeto.csv\n",
      "CNN_immigration_page1to5000_timeto.csv\n",
      "CNN_immigration.csv\n",
      "CNN_US_page1to1000_timeto.csv\n",
      "CNN_democratic.csv\n",
      "CNN_invasion_page1to5000_timeto.csv\n",
      "CNN_conflict.csv\n",
      "CNN_police_page1to5000_timeto.csv\n",
      "CNN_terror_page1to5000_timeto.csv\n",
      "CNN_violence.csv\n",
      "CNN_conflict_page1to10000_timeto.csv\n",
      "CNN_China_page1to1000_timeto.csv\n",
      "CNN_Russia_page1to1000_timeto.csv\n",
      "CNN_China.csv\n",
      "CNN_Russia.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences_cnn = []\n",
    "sentences_nypost = []\n",
    "\n",
    "counts_general = dict()\n",
    "counts_cnn = dict()\n",
    "counts_nypost = dict()\n",
    "counts_general[0] = 0\n",
    "counts_cnn[0] = 0\n",
    "counts_nypost[0] = 0\n",
    "for fname in os.listdir('./data/nypost/'):\n",
    "    if len(fname) > 12:\n",
    "        print(fname)\n",
    "        df = pd.read_csv(os.path.join('./data/nypost', fname))\n",
    "        for idx, row in df.iterrows():\n",
    "            for st in sent_tokenize(str(row['text'])):\n",
    "                sts_list = gensim.utils.simple_preprocess(st)\n",
    "                hist = Counter(sts_list)\n",
    "                for k in hist:\n",
    "                    counts_general[0] += 1\n",
    "                    counts_nypost[0] += 1\n",
    "                    counts_general[k] = hist[k] + counts_general.get(k, 0)\n",
    "                    counts_nypost[k] = hist[k] + counts_general.get(k, 0)\n",
    "                sentences_cnn.append(sts_list)\n",
    "\n",
    "\n",
    "for fname in os.listdir('./data/CNN/'):\n",
    "    if len(fname) > 12:\n",
    "        print(fname)\n",
    "        df = pd.read_csv(os.path.join('./data/CNN', fname))\n",
    "        for idx, row in df.iterrows():\n",
    "            for st in sent_tokenize(str(row['text'])):\n",
    "                sts_list = gensim.utils.simple_preprocess(st)\n",
    "                hist = Counter(sts_list)\n",
    "                for k in hist:\n",
    "                    counts_general[0] += 1\n",
    "                    counts_cnn[0] += 1\n",
    "                    counts_cnn[k] = hist[k] + counts_cnn.get(k, 0)\n",
    "                    counts_general[k] = hist[k] + counts_general.get(k, 0)\n",
    "                sentences_nypost.append(sts_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "json.dump(counts_general, open(os.path.join('./wv', 'general_word_freq_add.json'), 'w'))\n",
    "json.dump(counts_nypost, open(os.path.join('./wv', 'nypost_word_freq_add.json'), 'w'))\n",
    "json.dump(counts_cnn, open(os.path.join('./wv', 'cnn_word_freq_add.json'), 'w'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "opts = {'dims': 100,\n",
    "        'window': 5,\n",
    "        'n_cpu': -1,\n",
    "        'min_count': 30,\n",
    "        'vocab_size': 10000,\n",
    "        'sample': 0.0001,\n",
    "        'n_iter': 30\n",
    "        }\n",
    "model = gensim.models.Word2Vec(sentences=sentences_cnn + sentences_nypost,\n",
    "                               vector_size=opts['dims'],\n",
    "                               window=opts['window'],\n",
    "                               workers=opts['n_cpu'],\n",
    "                               sg=1,\n",
    "                               hs=0,\n",
    "                               negative=5,\n",
    "                               min_count=opts['min_count'],\n",
    "                               max_final_vocab=opts['vocab_size'],\n",
    "                               sample=opts['sample'],\n",
    "                               epochs=opts['n_iter']\n",
    "                               )\n",
    "model.save(os.path.join('./wv', 'general_add.model'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "opts = {'dims': 100,\n",
    "        'window': 5,\n",
    "        'n_cpu': -1,\n",
    "        'min_count': 30,\n",
    "        'vocab_size': 10000,\n",
    "        'sample': 0.0001,\n",
    "        'n_iter': 30\n",
    "        }\n",
    "model = gensim.models.Word2Vec(sentences=sentences_nypost,\n",
    "                               vector_size=opts['dims'],\n",
    "                               window=opts['window'],\n",
    "                               workers=opts['n_cpu'],\n",
    "                               sg=1,\n",
    "                               hs=0,\n",
    "                               negative=5,\n",
    "                               min_count=opts['min_count'],\n",
    "                               max_final_vocab=opts['vocab_size'],\n",
    "                               sample=opts['sample'],\n",
    "                               epochs=opts['n_iter']\n",
    "                               )\n",
    "\n",
    "model.save(os.path.join('./wv', 'nypost_add.model'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "opts = {'dims': 100,\n",
    "        'window': 5,\n",
    "        'n_cpu': -1,\n",
    "        'min_count': 30,\n",
    "        'vocab_size': 10000,\n",
    "        'sample': 0.0001,\n",
    "        'n_iter': 30\n",
    "        }\n",
    "model = gensim.models.Word2Vec(sentences=sentences_cnn,\n",
    "                               vector_size=opts['dims'],\n",
    "                               window=opts['window'],\n",
    "                               workers=opts['n_cpu'],\n",
    "                               sg=1,\n",
    "                               hs=0,\n",
    "                               negative=5,\n",
    "                               min_count=opts['min_count'],\n",
    "                               max_final_vocab=opts['vocab_size'],\n",
    "                               sample=opts['sample'],\n",
    "                               epochs=opts['n_iter']\n",
    "                               )\n",
    "\n",
    "model.save(os.path.join('./wv', 'cnn_add.model'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Model Aligning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from align import CCAAligner, get_cca_aligner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stopWords_nltk = set(stopwords.words('english'))\n",
    "stopWords_spc = {'those', 'on', 'own', '’ve', 'yourselves', 'around', 'between', 'four', 'been', 'alone', 'off', 'am',\n",
    "                 'then', 'other', 'can', 'regarding', 'hereafter', 'front', 'too', 'used', 'wherein', '‘ll', 'doing',\n",
    "                 'everything', 'up', 'onto', 'never', 'either', 'how', 'before', 'anyway', 'since', 'through', 'amount',\n",
    "                 'now', 'he', 'was', 'have', 'into', 'because', 'not', 'therefore', 'they', 'n’t', 'even', 'whom', 'it',\n",
    "                 'see', 'somewhere', 'thereupon', 'nothing', 'whereas', 'much', 'whenever', 'seem', 'until', 'whereby',\n",
    "                 'at', 'also', 'some', 'last', 'than', 'get', 'already', 'our', 'once', 'will', 'noone', \"'m\", 'that',\n",
    "                 'what', 'thus', 'no', 'myself', 'out', 'next', 'whatever', 'although', 'though', 'which', 'would',\n",
    "                 'therein', 'nor', 'somehow', 'whereupon', 'besides', 'whoever', 'ourselves', 'few', 'did', 'without',\n",
    "                 'third', 'anything', 'twelve', 'against', 'while', 'twenty', 'if', 'however', 'herself', 'when', 'may',\n",
    "                 'ours', 'six', 'done', 'seems', 'else', 'call', 'perhaps', 'had', 'nevertheless', 'where', 'otherwise',\n",
    "                 'still', 'within', 'its', 'for', 'together', 'elsewhere', 'throughout', 'of', 'others', 'show', '’s',\n",
    "                 'anywhere', 'anyhow', 'as', 'are', 'the', 'hence', 'something', 'hereby', 'nowhere', 'latterly', 'say',\n",
    "                 'does', 'neither', 'his', 'go', 'forty', 'put', 'their', 'by', 'namely', 'could', 'five', 'unless',\n",
    "                 'itself', 'is', 'nine', 'whereafter', 'down', 'bottom', 'thereby', 'such', 'both', 'she', 'become',\n",
    "                 'whole', 'who', 'yourself', 'every', 'thru', 'except', 'very', 'several', 'among', 'being', 'be',\n",
    "                 'mine', 'further', 'n‘t', 'here', 'during', 'why', 'with', 'just', \"'s\", 'becomes', '’ll', 'about',\n",
    "                 'a', 'using', 'seeming', \"'d\", \"'ll\", \"'re\", 'due', 'wherever', 'beforehand', 'fifty', 'becoming',\n",
    "                 'might', 'amongst', 'my', 'empty', 'thence', 'thereafter', 'almost', 'least', 'someone', 'often',\n",
    "                 'from', 'keep', 'him', 'or', '‘m', 'top', 'her', 'nobody', 'sometime', 'across', '‘s', '’re',\n",
    "                 'hundred', 'only', 'via', 'name', 'eight', 'three', 'back', 'to', 'all', 'became', 'move', 'me', 'we',\n",
    "                 'formerly', 'so', 'i', 'whence', 'under', 'always', 'himself', 'in', 'herein', 'more', 'after',\n",
    "                 'themselves', 'you', 'above', 'sixty', 'them', 'your', 'made', 'indeed', 'most', 'everywhere',\n",
    "                 'fifteen', 'but', 'must', 'along', 'beside', 'hers', 'side', 'former', 'anyone', 'full', 'has',\n",
    "                 'yours', 'whose', 'behind', 'please', 'ten', 'seemed', 'sometimes', 'should', 'over', 'take', 'each',\n",
    "                 'same', 'rather', 'really', 'latter', 'and', 'ca', 'hereupon', 'part', 'per', 'eleven', 'ever', '‘re',\n",
    "                 'enough', \"n't\", 'again', '‘d', 'us', 'yet', 'moreover', 'mostly', 'one', 'meanwhile', 'whither',\n",
    "                 'there', 'toward', '’m', \"'ve\", '’d', 'give', 'do', 'an', 'quite', 'these', 'everyone', 'towards',\n",
    "                 'this', 'cannot', 'afterwards', 'beyond', 'make', 'were', 'whether', 'well', 'another', 'below',\n",
    "                 'first', 'upon', 'any', 'none', 'many', 'serious', 'various', 're', 'two', 'less', '‘ve'}\n",
    "stopWords_s = stopWords_spc | stopWords_nltk | STOPWORDS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_general = gensim.models.Word2Vec.load(os.path.join('./models', 'general_add.model'))\n",
    "\n",
    "vocab = list(sorted(list(model_general.wv.index_to_key)))\n",
    "vocab = [w for w in vocab if w not in stopWords_s]\n",
    "mtx = np.vstack([model_general.wv[w] for w in vocab])\n",
    "\n",
    "clustering = KMeans(n_clusters=300).fit(mtx)\n",
    "res = {}\n",
    "t_align = {}\n",
    "for c, w in zip(clustering.labels_, vocab):\n",
    "    t_align[w] = c\n",
    "    c = str(c)\n",
    "    if c not in res:\n",
    "        res[c] = []\n",
    "    res[c].append(w)\n",
    "json.dump(res, open(os.path.join('./models', 'news_clustering_add.tpc'), 'w'))\n",
    "pickle.dump(t_align, open(os.path.join('./models', 't_align.pkl'), 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "c_num = []\n",
    "c_words = []\n",
    "c_name = []\n",
    "for k in res.keys():\n",
    "    c_num.append(k)\n",
    "    c_words.append(res[k])\n",
    "    c_name.append([])\n",
    "pd.DataFrame({'id': c_num, 'name': c_name, 'words': c_words}).to_csv('./news_topics_add.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_general=gensim.models.Word2Vec.load(os.path.join('./models', 'general_add.model'))\n",
    "model_nypost = gensim.models.Word2Vec.load(os.path.join('./models', 'nypost_add.model'))\n",
    "model_cnn = gensim.models.Word2Vec.load(os.path.join('./models', 'cnn_add.model'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shared_vocab = set(model_nypost.wv.key_to_index).intersection(set(model_cnn.wv.key_to_index)).intersection(set(model_general.wv.key_to_index))\n",
    "stopWords = list(stopWords_s & shared_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aligner_cnn = get_cca_aligner(model_cnn, model_general, stopWords)\n",
    "pickle.dump(aligner_cnn, open(os.path.join('./models', 'align_cnn_add.pkl'), 'wb'))\n",
    "aligner_nypost = get_cca_aligner(model_nypost, model_general, stopWords)\n",
    "pickle.dump(aligner_nypost, open(os.path.join('./models', 'align_nypost_add.pkl'), 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Analysis: Topic Description"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Word Cloud"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from analysis import Word_Cloud\n",
    "kargs2 = {'medias': ['CNN', 'nypost'],                  # medias to include\n",
    "          'keywords': keywords,                         # keywords to include\n",
    "          'root': '../data',                               # root directory to load the news\n",
    "          'limit': 1000,                                # number of news to load\n",
    "          'custom_stopwords':open('custom_stopwords.txt', 'r').read().split('\\n'),}  # add custom stopwords\n",
    "kargs2['custom_stopwords']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wc = Word_Cloud(**kargs2)\n",
    "wc.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Word CLuster Analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim import models,corpora\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from analysis import get_lemmas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get dataframe composed of all csvs in CNN folder\n",
    "csvs = [x for x in os.listdir('../data/CNN/') if x.endswith('.csv')]\n",
    "fns = [os.path.splitext(os.path.basename(x))[0] for x in csvs]\n",
    "topics=[]\n",
    "for i in fns:\n",
    "    topics.append(i.split('_')[1])\n",
    "d = {}\n",
    "for i in range(len(fns)):\n",
    "    d[topics[i]] = pd.read_csv(os.path.join('./CNN',csvs[i]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get pyLDAvis visualizations for all topic models under CNN\n",
    "cnn_topics=[]\n",
    "ps = []\n",
    "for i in fns:\n",
    "    cnn_topics.append(i.split('_')[1])\n",
    "\n",
    "for i in cnn_topics:\n",
    "    model = models.ldamodel.LdaModel.load('{}CNN.model'.format(i))\n",
    "    topics = model.print_topics(num_words=20)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "    lemmas = d[i]['text'].apply(get_lemmas)\n",
    "    dictionary = corpora.Dictionary.load('{}CNN.model.id2word'.format(i))\n",
    "    corpus = [dictionary.doc2bow(text) for text in lemmas]\n",
    "    pyLDAvis.enable_notebook()\n",
    "    p = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
    "    ps.append(p)\n",
    "    print(i+'done')\n",
    "    pyLDAvis.show(p)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#import nypost data\n",
    "ny_csvs = [x for x in os.listdir('../data/nypost/') if x.endswith('.csv')]\n",
    "fns = [os.path.splitext(os.path.basename(x))[0] for x in ny_csvs]\n",
    "ny_topics=[]\n",
    "for i in fns:\n",
    "    ny_topics.append(i.split('_')[1])\n",
    "ny_d = {}\n",
    "for i in range(len(fns)):\n",
    "    ny_d[ny_topics[i]] = pd.read_csv(os.path.join('./nypost',ny_csvs[i]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get pyLDAvis visualizations for all topic models under nypost\n",
    "ny_topics=[]\n",
    "ps_ny = []\n",
    "for i in fns:\n",
    "    ny_topics.append(i.split('_')[1])\n",
    "for i in cnn_topics:\n",
    "    model = models.ldamodel.LdaModel.load('{}nypost.model'.format(i))\n",
    "    topics = model.print_topics(num_words=20)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "    lemmas = d[i]['text'].apply(get_lemmas)\n",
    "    dictionary = corpora.Dictionary.load('{}nypost.model.id2word'.format(i))\n",
    "    corpus = [dictionary.doc2bow(text) for text in lemmas]\n",
    "    pyLDAvis.enable_notebook()\n",
    "    p = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
    "    ps_ny.append(p)\n",
    "    print(i+'done')\n",
    "    pyLDAvis.show(p)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Analysis: Content Coverage & Ideological Context"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forward_cnn = pickle.load(open(os.path.join('./models', 'align_cnn_add.pkl'), 'rb'))\n",
    "forward_nypost = pickle.load(open(os.path.join('./models', 'align_nypost_add.pkl'), 'rb'))\n",
    "t_align=pickle.load(open(os.path.join('./models', 't_align.pkl'), 'rb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from align import research_topic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topics = [\"attack\", \"Biden\", \"black\", \"China\", \"conflict\", \"crime\", \"democratic\", \"fair\", \"gun\", \"immigration\",\n",
    "          \"invasion\", \"LGBT\", \"police\", \"protest\", \"refused\", \"republic\", \"Russia\", \"terror\", \"Trump\", \"UK\", \"ukraine\",\n",
    "          \"US\", \"violence\", \"war\"]\n",
    "\n",
    "t_js_list = []\n",
    "t_cos_list = []\n",
    "mean_list = []\n",
    "t_cnn = []\n",
    "t_nypost = []\n",
    "\n",
    "for t in topics:\n",
    "    print(t)\n",
    "    a, b, c, d, e = research_topic(t,t_align,forward_cnn,forward_nypost,model_general,model_cnn,model_nypost)\n",
    "    t_js_list.append(a)\n",
    "    t_cos_list.append(b)\n",
    "    mean_list.append(c)\n",
    "    t_cnn.append(d)\n",
    "    t_nypost.append(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'mean_simi': mean_list,\n",
    "    'JS_div': t_js_list,\n",
    "    'Cos_div': t_cos_list,\n",
    "    't_cnn': t_cnn,\n",
    "    't_nypost': t_nypost\n",
    "},\n",
    "    index=topics\n",
    ").to_csv('./records_add.csv',index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "other = [1, 2, 3, 6, 7, 9, 11, 15, 16, 18, 19, 21]\n",
    "war = [10, 17, 20, 23]\n",
    "violence = [0, 4, 5, 8, 12, 13, 14, 22]\n",
    "\n",
    "df = pd.read_csv('./records.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 10])\n",
    "plt.scatter(np.array(df.iloc[other]['JS_div']), -np.array(df.iloc[other]['mean_simi']), c='b', label='other topics')\n",
    "plt.scatter(np.array(df.iloc[violence]['JS_div']), -np.array(df.iloc[violence]['mean_simi']), c='y',\n",
    "            label='violence topics')\n",
    "plt.scatter(np.array(df.iloc[war]['JS_div']), -np.array(df.iloc[war]['mean_simi']), c='r', label='war topics')\n",
    "plt.legend()\n",
    "for i in range(len(topics)):\n",
    "    plt.annotate(topics[i], (df['JS_div'][i], -df['mean_simi'][i]))\n",
    "plt.xlabel('Content coverage diversity by JS divergence')\n",
    "plt.ylabel('Ideological context diversity')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}