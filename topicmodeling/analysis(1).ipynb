{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import effective_n_jobs\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataframe composed of all csvs in CNN folder\n",
    "csvs = [x for x in os.listdir('../data/CNN/') if x.endswith('.csv')]\n",
    "fns = [os.path.splitext(os.path.basename(x))[0] for x in csvs]\n",
    "topics=[]\n",
    "for i in fns:\n",
    "    topics.append(i.split('_')[1])\n",
    "d = {}\n",
    "for i in range(len(fns)):\n",
    "    d[topics[i]] = pd.read_csv(os.path.join('../data/CNN',csvs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to lemmatize news texts\n",
    "STOP = set(nltk.corpus.stopwords.words('english') + list(string.punctuation) + ['``', \"''\", \"’\", \"“\", \"”\",\"–\", \"\\'s\"])\n",
    "\n",
    "def get_lemmas(text):\n",
    "    '''\n",
    "    Gets lemmas for a string input, excluding stop words, punctuation, as well\n",
    "    as a set of study-specific stop-words\n",
    "    '''\n",
    "    lemmas = [nltk.stem.WordNetLemmatizer().lemmatize(t)\n",
    "              for t in nltk.word_tokenize((str(text).lower())) if t not in STOP\n",
    "              ]\n",
    "    return lemmas\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=2):\n",
    "    '''\n",
    "    Computes Coherence values for LDA models with differing numbers of topics.\n",
    "\n",
    "    Returns list of models along with their respective coherence values (pick\n",
    "    models with the highest coherence)\n",
    "    '''\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                                 id2word=dictionary,\n",
    "                                                 num_topics=num_topics,\n",
    "                                                 workers=effective_n_jobs(-1))\n",
    "        model_list.append(model)\n",
    "        coherence_model = models.coherencemodel.CoherenceModel(model=model,\n",
    "                                                          corpus=corpus,\n",
    "                                                          dictionary=dictionary,\n",
    "                                                          coherence='u_mass')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        \n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda(topic_df, model_name, num_topics, workers):\n",
    "    '''\n",
    "    Trains LDA model on a dataframe of news articles, saves model to disk\n",
    "    Inputs:\n",
    "        topic_df: dataframe of news articles\n",
    "        model_name: name of model to be saved\n",
    "        num_topics: number of topics to be trained\n",
    "        workers: number of workers to be used in training\n",
    "    '''\n",
    "    # Get lemmas for each article\n",
    "    lemmas = topic_df['text'].apply(get_lemmas)\n",
    "    #reduce memory load\n",
    "    del topic_df\n",
    "    gc.collect()\n",
    "    # Initialize Series of lemmas as Gensim Dictionary for further processing\n",
    "    dictionary = corpora.Dictionary(lemmas)\n",
    "    # Convert dictionary into bag of words format: list of (token_id, token_count) tuples\n",
    "    bow_corpus = [dictionary.doc2bow(text) for text in lemmas]\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary=dictionary,\n",
    "                                                              corpus=bow_corpus,\n",
    "                                                              texts=lemmas,\n",
    "                                                              start=2,\n",
    "                                                              limit=40,\n",
    "                                                              step=6)\n",
    "    # train LDA model\n",
    "    ldamodel = models.ldamulticore.LdaMulticore(bow_corpus, num_topics=num_topics, id2word=dictionary, workers=workers, passes=20, iterations=400)\n",
    "    ldamodel.save('{}.model'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train all topic models for CNN, commented out because it takes a long time to run\n",
    "\n",
    "# for i in topics:\n",
    "#     train_lda(d[i], i+'CNN', 5, 10)\n",
    "#     print(i+'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pyLDAvis visualizations for all topic models under CNN\n",
    "cnn_topics=[]\n",
    "ps = []\n",
    "for i in fns:\n",
    "    cnn_topics.append(i.split('_')[1])\n",
    "for i in cnn_topics:\n",
    "    model = models.ldamodel.LdaModel.load('{}CNN.model'.format(i))\n",
    "    topics = model.print_topics(num_words=20)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "    lemmas = d[i]['text'].apply(get_lemmas)\n",
    "    dictionary = corpora.Dictionary.load('{}CNN.model.id2word'.format(i))\n",
    "    corpus = [dictionary.doc2bow(text) for text in lemmas]\n",
    "    pyLDAvis.enable_notebook()\n",
    "    p = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
    "    ps.append(p)\n",
    "    print(i+'done')\n",
    "#     else:\n",
    "#         model = models.ldamodel.LdaModel.load('{}cnn.model'.format(i))\n",
    "#         topics = ldamodel.print_topics(num_words=20)\n",
    "#         for topic in topics:\n",
    "#             print(topic)\n",
    "#         dictionary = corpora.Dictionary.load('{}cnn.id2word'.format(i))\n",
    "#         p = pyLDAvis.gensim_models.prepare(model, bow_corpus, dictionary)\n",
    "#         print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook\n",
    "ps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NyPost LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nypost data\n",
    "ny_csvs = [x for x in os.listdir('../data/nypost/') if x.endswith('.csv')]\n",
    "fns = [os.path.splitext(os.path.basename(x))[0] for x in ny_csvs]\n",
    "ny_topics=[]\n",
    "for i in fns:\n",
    "    ny_topics.append(i.split('_')[1])\n",
    "ny_d = {}\n",
    "for i in range(len(fns)):\n",
    "    ny_d[ny_topics[i]] = pd.read_csv(os.path.join('./nypost',ny_csvs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train all topic models for nypost, commented out because it takes a long time to run\n",
    "# for i in ny_topics:\n",
    "#     train_lda(ny_d[i], i+'nypost', 5, 10)\n",
    "#     print(i+'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain the following model because error before\n",
    "#train_lda(ny_d['UK'], 'UK'+'nypost', 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_lda(ny_d['ukraine'], 'ukraine'+'nypost', 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_lda(ny_d['US'], 'US'+'nypost', 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_lda(ny_d['violence'], 'violence'+'nypost', 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_lda(ny_d['war'], 'war'+'nypost', 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pyLDAvis visualizations for all topic models under nypost\n",
    "ny_topics=[]\n",
    "ps_ny = []\n",
    "for i in fns:\n",
    "    ny_topics.append(i.split('_')[1])\n",
    "for i in cnn_topics:\n",
    "    model = models.ldamodel.LdaModel.load('{}nypost.model'.format(i))\n",
    "    topics = model.print_topics(num_words=20)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "    lemmas = d[i]['text'].apply(get_lemmas)\n",
    "    dictionary = corpora.Dictionary.load('{}nypost.model.id2word'.format(i))\n",
    "    corpus = [dictionary.doc2bow(text) for text in lemmas]\n",
    "    pyLDAvis.enable_notebook()\n",
    "    p = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
    "    ps_ny.append(p)\n",
    "    print(i+'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display pyLDAvis visualizations for topic models under nypost\n",
    "ps_ny[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_ny[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# war\n",
    "ps[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_ny[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nypost terror\n",
    "ps_ny[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN terror\n",
    "ps[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ukraine CNN\n",
    "ps[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ukraine nypost\n",
    "ps_ny[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#violence CNN\n",
    "ps[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#violence nypost\n",
    "ps_ny[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gun CNN \n",
    "ps[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_ny[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# republic CNN\n",
    "ps[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# republic nypost\n",
    "ps_ny[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# democrat CNN\n",
    "ps[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# democrat nypost\n",
    "ps_ny[6]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac86d215dea9a0cdf4bbda73266032985b78d84038a3539ec44e8b053ab6c7d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
